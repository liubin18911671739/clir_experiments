# Cross-Lingual IR Experiments

**[English](#english) | [ä¸­æ–‡](#ä¸­æ–‡)**

Toolkit for NeuCLIR / CAKE-ILC style cross-lingual information retrieval experiments.

**Status**: âœ… Production Ready | **Version**: 2.6.0 | **Lines of Code**: ~5,329 ğŸš€ | **API**: FastAPI REST

---

## ä¸­æ–‡

### ç®€ä»‹

è¿™æ˜¯ä¸€ä¸ªç”¨äº NeuCLIR / CAKE-ILC é£æ ¼è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢å®éªŒçš„å·¥å…·åŒ…ã€‚

### æ ¸å¿ƒåŠŸèƒ½

- **ç¨€ç–æ£€ç´¢**: BM25 ä¼ ç»Ÿæ£€ç´¢ï¼ˆPyserini/Luceneï¼‰
- **å¯†é›†æ£€ç´¢**: mDPR é£æ ¼åŒç¼–ç å™¨å’Œ ColBERT æ™šæœŸäº¤äº’æ¨¡å‹
- **æ··åˆæ£€ç´¢**: RRFã€çº¿æ€§èåˆã€åŠ æƒèåˆã€CombSUMã€CombMNZ
- **ç¥ç»é‡æ’åº**: monoT5/mT5 åºåˆ—åˆ°åºåˆ—é‡æ’åºå™¨
- **æŸ¥è¯¢æ‰©å±•**: RM3 å’Œ PRF (Pseudo-Relevance Feedback) ğŸ†•
- **è‡ªåŠ¨è¯„ä¼°**: trec_eval é›†æˆï¼Œæ‰¹é‡è¯„ä¼°
- **æ‰¹é‡å®éªŒ**: ç«¯åˆ°ç«¯æµæ°´çº¿ç¼–æ’
- **REST API**: FastAPI åœ¨çº¿æ£€ç´¢æœåŠ¡ ğŸ†•
- **Docker éƒ¨ç½²**: å®¹å™¨åŒ–ç”Ÿäº§ç¯å¢ƒæ”¯æŒ ğŸ†•
- **æ€§èƒ½åŸºå‡†**: benchmark.py æ€§èƒ½åˆ†æå·¥å…· ğŸ†•
- **é…ç½®é©±åŠ¨**: æ‰€æœ‰è®¾ç½®é›†ä¸­åœ¨ `config/neuclir.yaml`
- **å•å…ƒæµ‹è¯•**: Pytest æµ‹è¯•å¥—ä»¶
- **TREC å…¼å®¹**: æ ‡å‡† TREC è¿è¡Œæ–‡ä»¶æ ¼å¼ï¼Œä¾¿äºè¯„ä¼°

### å¿«é€Ÿå¼€å§‹

```bash
# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# è·å–å®éªŒæ•°æ®ï¼ˆæ¨èä½¿ç”¨ NeuCLIR æ•°æ®é›†ï¼‰
pip install ir-datasets

# ä¸‹è½½å¹¶è½¬æ¢ NeuCLIR æ•°æ®ï¼ˆä»¥æ³¢æ–¯è¯­ä¸ºä¾‹ï¼‰
python -c "
import ir_datasets
import json
from pathlib import Path

dataset = ir_datasets.load('neuclir/1/fa')
Path('data/corpus/fas').mkdir(parents=True, exist_ok=True)

# è½¬æ¢è¯­æ–™åº“ä¸º JSONL
with open('data/corpus/fas/corpus.jsonl', 'w', encoding='utf-8') as f:
    for doc in dataset.docs_iter():
        json.dump({'id': doc.doc_id, 'contents': doc.title + ' ' + doc.text}, f, ensure_ascii=False)
        f.write('\n')

# å¯¼å‡ºä¸»é¢˜
with open('data/topics/fas.topics.txt', 'w', encoding='utf-8') as f:
    for topic in dataset.queries_iter():
        f.write(f'<top>\n<num> Number: {topic.query_id}\n<title> {topic.text}\n</top>\n\n')

# å¯¼å‡º qrels
with open('data/qrels/fas.qrels.txt', 'w', encoding='utf-8') as f:
    for qrel in dataset.qrels_iter():
        f.write(f'{qrel.query_id} 0 {qrel.doc_id} {qrel.relevance}\n')
"

# æ–¹å¼ 1: ä½¿ç”¨æ‰¹é‡å®éªŒè„šæœ¬ï¼ˆæ¨èï¼‰
python scripts/run_experiments.py --config config/neuclir.yaml --pipeline bm25

# æ–¹å¼ 2: æ‰‹åŠ¨è¿è¡Œå„æ­¥éª¤
# æ„å»º BM25 ç´¢å¼•
python scripts/build_index_bm25.py --config config/neuclir.yaml --lang fas

# è¿è¡Œæ£€ç´¢
python scripts/run_bm25.py --config config/neuclir.yaml --lang fas

# è‡ªåŠ¨è¯„ä¼°
python scripts/evaluate.py --config config/neuclir.yaml --run_dir runs/bm25 --lang fas

# è¿è¡Œæµ‹è¯•
pytest tests/
```

### æ•°æ®è·å–æŒ‡å—

#### æ–¹å¼ 1: ä½¿ç”¨ NeuCLIR æ•°æ®é›†ï¼ˆæ¨èï¼‰

NeuCLIR æ˜¯ TREC 2022-2023 çš„å®˜æ–¹è·¨è¯­è¨€æ£€ç´¢è¯„æµ‹æ•°æ®é›†ï¼š

- **å®˜ç½‘**: https://neuclir.github.io/
- **åŒ…å«è¯­è¨€**: æ³¢æ–¯è¯­(fa)ã€ä¿„è¯­(ru)ã€ä¸­æ–‡(zh)
- **æ•°æ®å†…å®¹**: æ–°é—»æ–‡ç« è¯­æ–™åº“ã€è‹±æ–‡æŸ¥è¯¢ã€äººå·¥æ ‡æ³¨çš„ç›¸å…³æ€§åˆ¤æ–­
- **è·å–æ–¹å¼**: ä½¿ç”¨ `ir-datasets` åº“è‡ªåŠ¨ä¸‹è½½

```bash
pip install ir-datasets

# ä¸‹è½½æ³¢æ–¯è¯­æ•°æ®
python -c "import ir_datasets; list(ir_datasets.load('neuclir/1/fa').docs_iter())"
```

#### æ–¹å¼ 2: ä½¿ç”¨ HC4 æ•°æ®é›†

HC4 (Human-translated CLIR Collection) æ˜¯å¦ä¸€ä¸ªä¼˜ç§€çš„æ•°æ®é›†ï¼š

```bash
pip install ir-datasets
python -c "import ir_datasets; list(ir_datasets.load('hc4/fa').docs_iter())"
```

#### æ–¹å¼ 3: ä½¿ç”¨è‡ªå·±çš„æ•°æ®

æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ç»„ç»‡æ•°æ®å³å¯ã€‚

### æ–‡æ¡£

- ğŸ“‹ [TODO.md](TODO.md) - å¼€å‘è¿›åº¦å’Œè®¡åˆ’
- ğŸ“– å®Œæ•´ä½¿ç”¨æ–‡æ¡£è§ä¸‹æ–¹è‹±æ–‡éƒ¨åˆ†
- ğŸ¤ [CONTRIBUTING.md](CONTRIBUTING.md) - è´¡çŒ®æŒ‡å—

---

## English

## Features

- **Sparse Retrieval**: BM25 traditional retrieval (Pyserini/Lucene)
- **Dense Retrieval**: mDPR-style dual encoders and ColBERT late interaction models
- **Hybrid Retrieval**: RRF, linear combination, weighted fusion, CombSUM, CombMNZ
- **Reranking**: monoT5/mT5 seq2seq rerankers
- **Query Expansion**: RM3 and PRF (Pseudo-Relevance Feedback) ğŸ†•
- **Automatic Evaluation**: trec_eval integration, batch evaluation
- **Batch Experiments**: End-to-end pipeline orchestration
- **REST API**: FastAPI-based online retrieval service ğŸ†•
- **Docker Support**: Containerized deployment for production ğŸ†•
- **Benchmarking**: Performance analysis and profiling tools ğŸ†•
- **Configuration-driven**: All settings in `config/neuclir.yaml`
- **Unit Tests**: Pytest test suite
- **TREC-compatible**: Standard TREC run file formats for evaluation

## Project Structure

```
clir_experiments/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ neuclir.yaml              # Main configuration file
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ corpus/{lang}/            # JSONL corpus files
â”‚   â”œâ”€â”€ topics/{lang}.topics.txt  # TREC topic files
â”‚   â””â”€â”€ qrels/{lang}.qrels.txt    # TREC qrels files
â”œâ”€â”€ indexes/
â”‚   â”œâ”€â”€ bm25/{lang}/              # BM25 indexes
â”‚   â””â”€â”€ dense/{index_name}_{lang}/ # Dense indexes
â”œâ”€â”€ runs/
â”‚   â”œâ”€â”€ bm25/                     # BM25 run files
â”‚   â”œâ”€â”€ dense/                    # Dense retrieval runs
â”‚   â””â”€â”€ reranked/                 # Reranked runs
â””â”€â”€ scripts/
    â”œâ”€â”€ utils_io.py               # I/O utilities
    â”œâ”€â”€ utils_topics.py           # Topic parsing utilities
    â”œâ”€â”€ build_index_dense.py      # Build dense indexes
    â”œâ”€â”€ run_dense_mdpr.py         # Run mDPR search
    â”œâ”€â”€ run_dense_colbert.py      # Run ColBERT search
    â””â”€â”€ rerank_mt5.py             # Rerank with monoT5/mT5
```

## Quick Start with API ğŸ†•

**Launch the REST API service:**

```bash
# Start API server
uvicorn api.main:app --reload --port 8000

# Or use Docker
docker-compose up -d clir-api

# Access interactive API docs
open http://localhost:8000/docs
```

**API Features:**
- BM25 search endpoint
- Dense retrieval endpoint
- Hybrid search with multiple fusion strategies
- Neural reranking endpoint
- Full OpenAPI/Swagger documentation

See [API Documentation](api/README.md) for details.

---

## Installation

### Requirements

- Python 3.9+
- CUDA 11+ (optional, for GPU acceleration)
- Docker & Docker Compose (optional, for containerized deployment)

### Setup

```bash
# Clone repository
git clone <repository-url>
cd clir_experiments

# Install dependencies
pip install -r requirements.txt

# For ColBERT support (optional)
pip install pyserini[colbert]
```

## Quick Start

### 1. Prepare Data

#### Option A: Use NeuCLIR Dataset (Recommended)

The **NeuCLIR** (TREC 2022-2023) dataset is publicly available for cross-lingual IR research:

**Download NeuCLIR data:**

```bash
# Visit NeuCLIR official website
# https://neuclir.github.io/

# Or download from IR Datasets
pip install ir-datasets

# Download Persian (Farsi) data
python -c "import ir_datasets; dataset = ir_datasets.load('neuclir/1/fa'); 
for doc in dataset.docs_iter(): print(doc)"

# Download Russian data
python -c "import ir_datasets; dataset = ir_datasets.load('neuclir/1/ru'); 
for doc in dataset.docs_iter(): print(doc)"

# Download Chinese data
python -c "import ir_datasets; dataset = ir_datasets.load('neuclir/1/zh'); 
for doc in dataset.docs_iter(): print(doc)"
```

**NeuCLIR Dataset includes:**
- **Corpora**: News articles in Persian, Russian, and Chinese
- **Topics**: English queries (50-100 topics per year)
- **Qrels**: Relevance judgments from TREC assessors
- **Years**: 2022, 2023 data available

**Convert NeuCLIR to JSONL format:**

```python
import ir_datasets
import json
from pathlib import Path

# Load dataset
dataset = ir_datasets.load('neuclir/1/fa')  # or 'neuclir/1/ru', 'neuclir/1/zh'

# Create output directory
output_dir = Path('data/corpus/fas')
output_dir.mkdir(parents=True, exist_ok=True)

# Convert to JSONL
with open(output_dir / 'corpus.jsonl', 'w', encoding='utf-8') as f:
    for doc in dataset.docs_iter():
        json.dump({
            'id': doc.doc_id,
            'contents': doc.title + ' ' + doc.text
        }, f, ensure_ascii=False)
        f.write('\n')

# Export topics
with open('data/topics/fas.topics.txt', 'w', encoding='utf-8') as f:
    for topic in dataset.queries_iter():
        f.write(f"<top>\n<num> Number: {topic.query_id}\n")
        f.write(f"<title> {topic.text}\n</top>\n\n")

# Export qrels
with open('data/qrels/fas.qrels.txt', 'w', encoding='utf-8') as f:
    for qrel in dataset.qrels_iter():
        f.write(f"{qrel.query_id} 0 {qrel.doc_id} {qrel.relevance}\n")
```

#### Option B: Use HC4 Dataset

**HC4** (Human-translated CLIR Collection) is another excellent dataset:

```bash
# Download HC4 data using ir_datasets
pip install ir_datasets

# Available languages: Persian (fa), Russian (ru), Chinese (zh)
python -c "import ir_datasets; dataset = ir_datasets.load('hc4/fa'); 
for doc in dataset.docs_iter(): print(doc)"
```

#### Option C: Use Your Own Data

Organize your data following this structure:

```bash
# Corpus files (JSONL format)
data/corpus/fas/*.jsonl
data/corpus/rus/*.jsonl
data/corpus/zho/*.jsonl

# Topic files (TREC format)
data/topics/fas.topics.txt
data/topics/rus.topics.txt
data/topics/zho.topics.txt

# Qrels files (TREC format)
data/qrels/fas.qrels.txt
data/qrels/rus.qrels.txt
data/qrels/zho.qrels.txt
```

**Corpus format** (JSONL):
```json
{"id": "doc001", "contents": "Document text here..."}
{"id": "doc002", "contents": "Another document..."}
```

**Topics format** (TREC-style):
```xml
<top>
<num> Number: 1
<title> Query text here
</top>
```

Or simple format:
```
1    Query text here
2    Another query
```

**Qrels format** (TREC):
```
1 0 doc001 1
1 0 doc005 2
2 0 doc003 1
```

### 2. Configure Experiment

Edit `config/neuclir.yaml` to set:
- Languages to process
- Model names (mDPR, ColBERT, monoT5/mT5)
- Retrieval parameters (top-k, batch sizes)
- Hardware settings (GPU device, threads)

### 3. Build Dense Indexes

Build mDPR-style dense index:
```bash
python scripts/build_index_dense.py \
    --config config/neuclir.yaml \
    --model mdpr \
    --lang fas
```

Build ColBERT index:
```bash
python scripts/build_index_dense.py \
    --config config/neuclir.yaml \
    --model colbert \
    --lang rus
```

### 4. Run Dense Retrieval

Search with mDPR:
```bash
python scripts/run_dense_mdpr.py \
    --config config/neuclir.yaml \
    --lang fas
```

Search with ColBERT:
```bash
python scripts/run_dense_colbert.py \
    --config config/neuclir.yaml \
    --lang rus
```

### 5. Rerank Results

Rerank with monoT5/mT5:
```bash
python scripts/rerank_mt5.py \
    --config config/neuclir.yaml \
    --base_run runs/dense/mdpr_fas.run \
    --lang fas
```

Use multilingual mT5:
```bash
python scripts/rerank_mt5.py \
    --config config/neuclir.yaml \
    --base_run runs/dense/mdpr_rus.run \
    --lang rus \
    --model mt5_multilingual
```

### 6. Evaluate Results

Use TREC `trec_eval`:
```bash
trec_eval -m ndcg_cut.10 \
    data/qrels/fas.qrels.txt \
    runs/reranked/mdpr_fas_mt5.run
```

## Configuration

### Model Configuration

Configure models in `config/neuclir.yaml`:

**mDPR models**:
```yaml
dense:
  mdpr:
    model_name: "facebook/mdpr-question_encoder-base-nq"
    doc_encoder: "facebook/mdpr-ctx_encoder-base-nq"
    query_encoder: "facebook/mdpr-question_encoder-base-nq"
    embedding_dim: 768
```

**ColBERT models**:
```yaml
dense:
  colbert:
    model_name: "colbert-ir/colbertv2.0"
    max_doc_length: 512
    max_query_length: 128
```

**Reranking models**:
```yaml
reranking:
  mt5:
    model_name: "castorini/monot5-base-msmarco-10k"
    batch_size: 32
    top_k: 100
```

## Advanced Usage

### Custom Model Paths

To use a local or custom model:

```yaml
dense:
  mdpr:
    doc_encoder: "/path/to/local/model"
    query_encoder: "/path/to/local/model"
```

### GPU Configuration

Configure GPU usage:

```yaml
system:
  use_gpu: true
  gpu_device: 0  # CUDA device ID
```

For reranking with mixed precision:

```yaml
reranking:
  mt5:
    device: "cuda"
    use_fp16: true  # Use FP16 for faster inference
```

### Batch Processing

Process multiple languages:

```bash
for lang in fas rus zho; do
    python scripts/build_index_dense.py \
        --config config/neuclir.yaml \
        --model mdpr \
        --lang $lang

    python scripts/run_dense_mdpr.py \
        --config config/neuclir.yaml \
        --lang $lang
done
```

## Pipeline Examples

### BM25 Retrieval Pipeline ğŸ†•

```bash
# 1. Build BM25 index
python scripts/build_index_bm25.py \
    --config config/neuclir.yaml \
    --lang fas

# 2. Run BM25 search
python scripts/run_bm25.py \
    --config config/neuclir.yaml \
    --lang fas

# 3. Evaluate
python scripts/evaluate.py \
    --config config/neuclir.yaml \
    --run_dir runs/bm25 \
    --lang fas
```

### Dense Retrieval Pipeline

```bash
# 1. Build dense index
python scripts/build_index_dense.py \
    --config config/neuclir.yaml \
    --model mdpr \
    --lang fas

# 2. Run dense retrieval
python scripts/run_dense_mdpr.py \
    --config config/neuclir.yaml \
    --lang fas

# 3. Evaluate
python scripts/evaluate.py \
    --config config/neuclir.yaml \
    --run runs/dense/mdpr_fas.run \
    --lang fas
```

### Hybrid Retrieval Pipeline ğŸ†•

Combine BM25 and dense retrieval for better results:

```bash
# Method 1: Reciprocal Rank Fusion (RRF)
python scripts/run_hybrid.py \
    --config config/neuclir.yaml \
    --bm25_run runs/bm25/bm25_fas.run \
    --dense_run runs/dense/mdpr_fas.run \
    --lang fas \
    --method rrf

# Method 2: Weighted Fusion (70% BM25, 30% Dense)
python scripts/run_hybrid.py \
    --config config/neuclir.yaml \
    --bm25_run runs/bm25/bm25_fas.run \
    --dense_run runs/dense/mdpr_fas.run \
    --lang fas \
    --method weighted \
    --alpha 0.7
```

### Complete End-to-End Pipeline

With reranking and evaluation:

```bash
# 1. Run BM25
python scripts/run_bm25.py --config config/neuclir.yaml --lang fas

# 2. Rerank with monoT5
python scripts/rerank_mt5.py \
    --config config/neuclir.yaml \
    --base_run runs/bm25/bm25_fas.run \
    --lang fas

# 3. Evaluate reranked results
python scripts/evaluate.py \
    --config config/neuclir.yaml \
    --run runs/reranked/bm25_fas_mt5.run \
    --lang fas
```

### Batch Experiments ğŸ†•

Run complete pipelines automatically:

```bash
# Run full BM25 pipeline for all languages
python scripts/run_experiments.py \
    --config config/neuclir.yaml \
    --pipeline bm25

# Run full dense + reranking pipeline
python scripts/run_experiments.py \
    --config config/neuclir.yaml \
    --pipeline dense_mdpr

# Run everything: BM25 + Dense + Hybrid + Reranking + Evaluation
python scripts/run_experiments.py \
    --config config/neuclir.yaml \
    --pipeline full
```

### Query Expansion ğŸ†•

Improve retrieval effectiveness with query expansion:

```bash
# RM3 query expansion
python scripts/query_expansion.py \
    --config config/neuclir.yaml \
    --base_run runs/bm25/bm25_fas.run \
    --lang fas \
    --method rm3 \
    --fb_docs 10 \
    --fb_terms 10 \
    --original_query_weight 0.5

# Pseudo-Relevance Feedback (PRF)
python scripts/query_expansion.py \
    --config config/neuclir.yaml \
    --base_run runs/bm25/bm25_fas.run \
    --lang fas \
    --method prf \
    --fb_docs 20 \
    --fb_terms 15
```

### Results Visualization ğŸ†•

Generate comparison reports:

```bash
python scripts/visualize_results.py \
    --results eval_results/*.json \
    --output reports/comparison.md
```

### Performance Benchmarking ğŸ†•

Analyze system performance:

```bash
python scripts/benchmark.py \
    --config config/neuclir.yaml \
    --mode index \
    --lang fas
```

### REST API Service ğŸ†•

Deploy as a web service:

```bash
# Development mode
uvicorn api.main:app --reload --port 8000

# Production mode
uvicorn api.main:app --host 0.0.0.0 --port 8000 --workers 4

# Docker deployment
docker-compose up -d clir-api

# GPU-enabled deployment
docker-compose --profile gpu up -d clir-api-gpu
```

**API Endpoints:**
- `GET /` - Health check
- `POST /search/bm25` - BM25 retrieval
- `POST /search/dense` - Dense retrieval
- `POST /search/hybrid` - Hybrid search
- `POST /rerank` - Neural reranking

See [API Documentation](api/README.md) for detailed usage.

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CLIR Experiments System                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚   Data Layer â”‚     â”‚ Config Layer â”‚                     â”‚
â”‚  â”‚              â”‚     â”‚              â”‚                     â”‚
â”‚  â”‚ â€¢ Corpus     â”‚     â”‚ neuclir.yaml â”‚                     â”‚
â”‚  â”‚ â€¢ Topics     â”‚     â”‚              â”‚                     â”‚
â”‚  â”‚ â€¢ Qrels      â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚            Retrieval Engines                          â”‚  â”‚
â”‚  â”‚                                                        â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚  â”‚
â”‚  â”‚  â”‚  BM25   â”‚  â”‚  mDPR   â”‚  â”‚ColBERT â”‚             â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚  â”‚
â”‚  â”‚                                                        â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚  â”‚
â”‚  â”‚  â”‚    Hybrid Fusion (RRF/CombSUM)     â”‚             â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚  â”‚
â”‚  â”‚                                                        â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚  â”‚
â”‚  â”‚  â”‚   Neural Reranking (monoT5/mT5)    â”‚             â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚             Enhancement Modules                       â”‚  â”‚
â”‚  â”‚                                                        â”‚  â”‚
â”‚  â”‚  â€¢ Query Expansion (RM3/PRF)                         â”‚  â”‚
â”‚  â”‚  â€¢ Evaluation (trec_eval)                            â”‚  â”‚
â”‚  â”‚  â€¢ Visualization                                      â”‚  â”‚
â”‚  â”‚  â€¢ Benchmarking                                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              REST API Layer (FastAPI)                 â”‚  â”‚
â”‚  â”‚                                                        â”‚  â”‚
â”‚  â”‚  â€¢ BM25 endpoint                                      â”‚  â”‚
â”‚  â”‚  â€¢ Dense endpoint                                     â”‚  â”‚
â”‚  â”‚  â€¢ Hybrid endpoint                                    â”‚  â”‚
â”‚  â”‚  â€¢ Rerank endpoint                                    â”‚  â”‚
â”‚  â”‚  â€¢ OpenAPI docs                                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚             Deployment Layer                          â”‚  â”‚
â”‚  â”‚                                                        â”‚  â”‚
â”‚  â”‚  â€¢ Docker containerization                            â”‚  â”‚
â”‚  â”‚  â€¢ GPU support                                        â”‚  â”‚
â”‚  â”‚  â€¢ Multi-worker deployment                            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Testing ğŸ†•

Run unit tests:

```bash
# Run all tests
pytest tests/

# Run specific test file with verbose output
pytest tests/test_utils_io.py -v

# Run specific test function
pytest tests/test_hybrid.py::test_reciprocal_rank_fusion -v
```

## Troubleshooting

### CUDA Out of Memory

Reduce batch sizes in config:

```yaml
dense:
  mdpr:
    batch_size: 64  # Reduce from 128

reranking:
  mt5:
    batch_size: 16  # Reduce from 32
```

### Missing Dependencies

Install specific extras:

```bash
# For ColBERT
pip install pyserini[colbert]

# For FAISS GPU support
pip install faiss-gpu
```

### Index Not Found

Ensure you've built the index before searching:

```bash
# Check if index exists
ls indexes/dense/mdpr_fas/

# If not, build it first
python scripts/build_index_dense.py --config config/neuclir.yaml --model mdpr --lang fas
```

## Citation

If you use this toolkit, please cite:

```bibtex
@misc{clir_experiments,
  title={Cross-Lingual IR Experiments Toolkit},
  author={Your Name},
  year={2025},
  url={https://github.com/yourusername/clir_experiments}
}
```

## What's New in v2.6.0 ğŸ‰

This version represents a major milestone with complete production-ready features:

### New Features
- âœ¨ **REST API Service**: FastAPI-based web service with 5 endpoints
- âœ¨ **Query Expansion**: RM3 and Pseudo-Relevance Feedback implementations
- âœ¨ **Enhanced Fusion**: CombSUM and CombMNZ strategies added
- âœ¨ **Results Visualization**: Markdown tables and ASCII charts
- âœ¨ **Performance Benchmarking**: Comprehensive profiling tools
- âœ¨ **Docker Support**: Full containerization with GPU support

### Statistics
- **Total Lines of Code**: 5,329 (238% increase from initial version)
- **Python Scripts**: 15 modules
- **API Endpoints**: 5 REST endpoints
- **Test Coverage**: 15+ unit tests
- **Fusion Strategies**: 5 methods (RRF, Linear, Weighted, CombSUM, CombMNZ)
- **Deployment Options**: Local, Docker, Docker+GPU

### Architecture Improvements
- Modular design with clear separation of concerns
- Comprehensive error handling and logging
- Production-ready API with OpenAPI documentation
- Docker multi-stage builds for optimized images
- GPU-accelerated deployment support

### Production Features
- âœ… REST API with Swagger/ReDoc documentation
- âœ… Docker containerization (CPU and GPU variants)
- âœ… Automated testing suite
- âœ… Comprehensive benchmarking tools
- âœ… Complete evaluation pipeline
- âœ… Result visualization and reporting

This toolkit is now suitable for both academic research and production deployment!

## License

MIT License - see LICENSE file for details.

## References

- [Pyserini](https://github.com/castorini/pyserini)
- [ColBERT](https://github.com/stanford-futuredata/ColBERT)
- [monoT5](https://github.com/castorini/pygaggle)
- [NeuCLIR](https://neuclir.github.io/)
# clir_experiments
