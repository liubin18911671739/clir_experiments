# Cross-Lingual IR Experiments

**[English](#english) | [ä¸­æ–‡](#ä¸­æ–‡)**

Toolkit for NeuCLIR / CAKE-ILC style cross-lingual information retrieval experiments.

**Status**: âœ… Production Ready | **Version**: 2.5.0 | **Lines of Code**: ~4,077 ğŸš€

---

## ä¸­æ–‡

### ç®€ä»‹

è¿™æ˜¯ä¸€ä¸ªç”¨äº NeuCLIR / CAKE-ILC é£æ ¼è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢å®éªŒçš„å·¥å…·åŒ…ã€‚

### æ ¸å¿ƒåŠŸèƒ½

- **ç¨€ç–æ£€ç´¢**: BM25 ä¼ ç»Ÿæ£€ç´¢ï¼ˆPyserini/Luceneï¼‰ğŸ†•
- **å¯†é›†æ£€ç´¢**: mDPR é£æ ¼åŒç¼–ç å™¨å’Œ ColBERT æ™šæœŸäº¤äº’æ¨¡å‹
- **æ··åˆæ£€ç´¢**: RRFã€çº¿æ€§èåˆã€åŠ æƒèåˆ ğŸ†•
- **ç¥ç»é‡æ’åº**: monoT5/mT5 åºåˆ—åˆ°åºåˆ—é‡æ’åºå™¨
- **è‡ªåŠ¨è¯„ä¼°**: trec_eval é›†æˆï¼Œæ‰¹é‡è¯„ä¼° ğŸ†•
- **æ‰¹é‡å®éªŒ**: ç«¯åˆ°ç«¯æµæ°´çº¿ç¼–æ’ ğŸ†•
- **é…ç½®é©±åŠ¨**: æ‰€æœ‰è®¾ç½®é›†ä¸­åœ¨ `config/neuclir.yaml`
- **å•å…ƒæµ‹è¯•**: Pytest æµ‹è¯•å¥—ä»¶ ğŸ†•
- **TREC å…¼å®¹**: æ ‡å‡† TREC è¿è¡Œæ–‡ä»¶æ ¼å¼ï¼Œä¾¿äºè¯„ä¼°

### å¿«é€Ÿå¼€å§‹

```bash
# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# æ–¹å¼ 1: ä½¿ç”¨æ‰¹é‡å®éªŒè„šæœ¬ï¼ˆæ¨èï¼‰ğŸ†•
python scripts/run_experiments.py --config config/neuclir.yaml --pipeline bm25

# æ–¹å¼ 2: æ‰‹åŠ¨è¿è¡Œå„æ­¥éª¤
# æ„å»º BM25 ç´¢å¼•
python scripts/build_index_bm25.py --config config/neuclir.yaml --lang fas

# è¿è¡Œæ£€ç´¢
python scripts/run_bm25.py --config config/neuclir.yaml --lang fas

# è‡ªåŠ¨è¯„ä¼°
python scripts/evaluate.py --config config/neuclir.yaml --run_dir runs/bm25 --lang fas

# è¿è¡Œæµ‹è¯•
pytest tests/
```

### æ–‡æ¡£

- ğŸ“‹ [TODO.md](TODO.md) - å¼€å‘è¿›åº¦å’Œè®¡åˆ’
- ğŸ“– å®Œæ•´ä½¿ç”¨æ–‡æ¡£è§ä¸‹æ–¹è‹±æ–‡éƒ¨åˆ†
- ğŸ¤ [CONTRIBUTING.md](CONTRIBUTING.md) - è´¡çŒ®æŒ‡å—

---

## English

## Features

- **Sparse Retrieval**: BM25 traditional retrieval (Pyserini/Lucene) ğŸ†•
- **Dense Retrieval**: mDPR-style dual encoders and ColBERT late interaction models
- **Hybrid Retrieval**: RRF, linear combination, weighted fusion ğŸ†•
- **Reranking**: monoT5/mT5 seq2seq rerankers
- **Automatic Evaluation**: trec_eval integration, batch evaluation ğŸ†•
- **Batch Experiments**: End-to-end pipeline orchestration ğŸ†•
- **Configuration-driven**: All settings in `config/neuclir.yaml`
- **Unit Tests**: Pytest test suite ğŸ†•
- **TREC-compatible**: Standard TREC run file formats for evaluation

## Project Structure

```
clir_experiments/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ neuclir.yaml              # Main configuration file
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ corpus/{lang}/            # JSONL corpus files
â”‚   â”œâ”€â”€ topics/{lang}.topics.txt  # TREC topic files
â”‚   â””â”€â”€ qrels/{lang}.qrels.txt    # TREC qrels files
â”œâ”€â”€ indexes/
â”‚   â”œâ”€â”€ bm25/{lang}/              # BM25 indexes
â”‚   â””â”€â”€ dense/{index_name}_{lang}/ # Dense indexes
â”œâ”€â”€ runs/
â”‚   â”œâ”€â”€ bm25/                     # BM25 run files
â”‚   â”œâ”€â”€ dense/                    # Dense retrieval runs
â”‚   â””â”€â”€ reranked/                 # Reranked runs
â””â”€â”€ scripts/
    â”œâ”€â”€ utils_io.py               # I/O utilities
    â”œâ”€â”€ utils_topics.py           # Topic parsing utilities
    â”œâ”€â”€ build_index_dense.py      # Build dense indexes
    â”œâ”€â”€ run_dense_mdpr.py         # Run mDPR search
    â”œâ”€â”€ run_dense_colbert.py      # Run ColBERT search
    â””â”€â”€ rerank_mt5.py             # Rerank with monoT5/mT5
```

## Installation

### Requirements

- Python 3.9+
- CUDA 11+ (optional, for GPU acceleration)

### Setup

```bash
# Clone repository
git clone <repository-url>
cd clir_experiments

# Install dependencies
pip install -r requirements.txt

# For ColBERT support (optional)
pip install pyserini[colbert]
```

## Quick Start

### 1. Prepare Data

Organize your data following this structure:

```bash
# Corpus files (JSONL format)
data/corpus/fas/*.jsonl
data/corpus/rus/*.jsonl
data/corpus/zho/*.jsonl

# Topic files (TREC format)
data/topics/fas.topics.txt
data/topics/rus.topics.txt
data/topics/zho.topics.txt

# Qrels files (TREC format)
data/qrels/fas.qrels.txt
data/qrels/rus.qrels.txt
data/qrels/zho.qrels.txt
```

**Corpus format** (JSONL):
```json
{"id": "doc001", "contents": "Document text here..."}
{"id": "doc002", "contents": "Another document..."}
```

**Topics format** (TREC-style):
```xml
<top>
<num> Number: 1
<title> Query text here
</top>
```

Or simple format:
```
1    Query text here
2    Another query
```

**Qrels format** (TREC):
```
1 0 doc001 1
1 0 doc005 2
2 0 doc003 1
```

### 2. Configure Experiment

Edit `config/neuclir.yaml` to set:
- Languages to process
- Model names (mDPR, ColBERT, monoT5/mT5)
- Retrieval parameters (top-k, batch sizes)
- Hardware settings (GPU device, threads)

### 3. Build Dense Indexes

Build mDPR-style dense index:
```bash
python scripts/build_index_dense.py \
    --config config/neuclir.yaml \
    --model mdpr \
    --lang fas
```

Build ColBERT index:
```bash
python scripts/build_index_dense.py \
    --config config/neuclir.yaml \
    --model colbert \
    --lang rus
```

### 4. Run Dense Retrieval

Search with mDPR:
```bash
python scripts/run_dense_mdpr.py \
    --config config/neuclir.yaml \
    --lang fas
```

Search with ColBERT:
```bash
python scripts/run_dense_colbert.py \
    --config config/neuclir.yaml \
    --lang rus
```

### 5. Rerank Results

Rerank with monoT5/mT5:
```bash
python scripts/rerank_mt5.py \
    --config config/neuclir.yaml \
    --base_run runs/dense/mdpr_fas.run \
    --lang fas
```

Use multilingual mT5:
```bash
python scripts/rerank_mt5.py \
    --config config/neuclir.yaml \
    --base_run runs/dense/mdpr_rus.run \
    --lang rus \
    --model mt5_multilingual
```

### 6. Evaluate Results

Use TREC `trec_eval`:
```bash
trec_eval -m ndcg_cut.10 \
    data/qrels/fas.qrels.txt \
    runs/reranked/mdpr_fas_mt5.run
```

## Configuration

### Model Configuration

Configure models in `config/neuclir.yaml`:

**mDPR models**:
```yaml
dense:
  mdpr:
    model_name: "facebook/mdpr-question_encoder-base-nq"
    doc_encoder: "facebook/mdpr-ctx_encoder-base-nq"
    query_encoder: "facebook/mdpr-question_encoder-base-nq"
    embedding_dim: 768
```

**ColBERT models**:
```yaml
dense:
  colbert:
    model_name: "colbert-ir/colbertv2.0"
    max_doc_length: 512
    max_query_length: 128
```

**Reranking models**:
```yaml
reranking:
  mt5:
    model_name: "castorini/monot5-base-msmarco-10k"
    batch_size: 32
    top_k: 100
```

## Advanced Usage

### Custom Model Paths

To use a local or custom model:

```yaml
dense:
  mdpr:
    doc_encoder: "/path/to/local/model"
    query_encoder: "/path/to/local/model"
```

### GPU Configuration

Configure GPU usage:

```yaml
system:
  use_gpu: true
  gpu_device: 0  # CUDA device ID
```

For reranking with mixed precision:

```yaml
reranking:
  mt5:
    device: "cuda"
    use_fp16: true  # Use FP16 for faster inference
```

### Batch Processing

Process multiple languages:

```bash
for lang in fas rus zho; do
    python scripts/build_index_dense.py \
        --config config/neuclir.yaml \
        --model mdpr \
        --lang $lang

    python scripts/run_dense_mdpr.py \
        --config config/neuclir.yaml \
        --lang $lang
done
```

## Pipeline Examples

### BM25 Retrieval Pipeline ğŸ†•

```bash
# 1. Build BM25 index
python scripts/build_index_bm25.py \
    --config config/neuclir.yaml \
    --lang fas

# 2. Run BM25 search
python scripts/run_bm25.py \
    --config config/neuclir.yaml \
    --lang fas

# 3. Evaluate
python scripts/evaluate.py \
    --config config/neuclir.yaml \
    --run_dir runs/bm25 \
    --lang fas
```

### Dense Retrieval Pipeline

```bash
# 1. Build dense index
python scripts/build_index_dense.py \
    --config config/neuclir.yaml \
    --model mdpr \
    --lang fas

# 2. Run dense retrieval
python scripts/run_dense_mdpr.py \
    --config config/neuclir.yaml \
    --lang fas

# 3. Evaluate
python scripts/evaluate.py \
    --config config/neuclir.yaml \
    --run runs/dense/mdpr_fas.run \
    --lang fas
```

### Hybrid Retrieval Pipeline ğŸ†•

Combine BM25 and dense retrieval for better results:

```bash
# Method 1: Reciprocal Rank Fusion (RRF)
python scripts/run_hybrid.py \
    --config config/neuclir.yaml \
    --bm25_run runs/bm25/bm25_fas.run \
    --dense_run runs/dense/mdpr_fas.run \
    --lang fas \
    --method rrf

# Method 2: Weighted Fusion (70% BM25, 30% Dense)
python scripts/run_hybrid.py \
    --config config/neuclir.yaml \
    --bm25_run runs/bm25/bm25_fas.run \
    --dense_run runs/dense/mdpr_fas.run \
    --lang fas \
    --method weighted \
    --alpha 0.7
```

### Complete End-to-End Pipeline

With reranking and evaluation:

```bash
# 1. Run BM25
python scripts/run_bm25.py --config config/neuclir.yaml --lang fas

# 2. Rerank with monoT5
python scripts/rerank_mt5.py \
    --config config/neuclir.yaml \
    --base_run runs/bm25/bm25_fas.run \
    --lang fas

# 3. Evaluate reranked results
python scripts/evaluate.py \
    --config config/neuclir.yaml \
    --run runs/reranked/bm25_fas_mt5.run \
    --lang fas
```

### Batch Experiments ğŸ†•

Run complete pipelines automatically:

```bash
# Run full BM25 pipeline for all languages
python scripts/run_experiments.py \
    --config config/neuclir.yaml \
    --pipeline bm25

# Run full dense + reranking pipeline
python scripts/run_experiments.py \
    --config config/neuclir.yaml \
    --pipeline dense_mdpr

# Run everything: BM25 + Dense + Hybrid + Reranking + Evaluation
python scripts/run_experiments.py \
    --config config/neuclir.yaml \
    --pipeline full
```

## Testing ğŸ†•

Run unit tests:

```bash
# Run all tests
pytest tests/

# Run specific test file with verbose output
pytest tests/test_utils_io.py -v

# Run specific test function
pytest tests/test_hybrid.py::test_reciprocal_rank_fusion -v
```

## Troubleshooting

### CUDA Out of Memory

Reduce batch sizes in config:

```yaml
dense:
  mdpr:
    batch_size: 64  # Reduce from 128

reranking:
  mt5:
    batch_size: 16  # Reduce from 32
```

### Missing Dependencies

Install specific extras:

```bash
# For ColBERT
pip install pyserini[colbert]

# For FAISS GPU support
pip install faiss-gpu
```

### Index Not Found

Ensure you've built the index before searching:

```bash
# Check if index exists
ls indexes/dense/mdpr_fas/

# If not, build it first
python scripts/build_index_dense.py --config config/neuclir.yaml --model mdpr --lang fas
```

## Citation

If you use this toolkit, please cite:

```bibtex
@misc{clir_experiments,
  title={Cross-Lingual IR Experiments Toolkit},
  author={Your Name},
  year={2025},
  url={https://github.com/yourusername/clir_experiments}
}
```

## License

MIT License - see LICENSE file for details.

## References

- [Pyserini](https://github.com/castorini/pyserini)
- [ColBERT](https://github.com/stanford-futuredata/ColBERT)
- [monoT5](https://github.com/castorini/pygaggle)
- [NeuCLIR](https://neuclir.github.io/)
# clir_experiments
